{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_vua_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMeJ7wGaTeW6shTVJ5J1U5B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OorRP8DmGiVJ","executionInfo":{"status":"ok","timestamp":1647116524871,"user_tz":360,"elapsed":13798,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"387e3d38-99f4-4efc-c85c-3c431ca03a06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","ROOT = '/content/drive'\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","source":["import os\n","import sys\n","from os.path import join \n","repo_dir = '/content/drive/MyDrive/gong-metaphor-detection'"],"metadata":{"id":"jdbJjOe9G9T1","executionInfo":{"status":"ok","timestamp":1647116542829,"user_tz":360,"elapsed":139,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# pip install requirements (takes a while)\n","#!cd drive/MyDrive/Repos/gong-metaphor-detection/; pip install -r requirements.txt"],"metadata":{"id":"uxx58pzZHJ8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install 'transformers==3.5.1'"],"metadata":{"id":"tdLjjXFJKdV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch==1.4.0"],"metadata":{"id":"KDgam-CNYsOa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvWsFdCwjoOy","executionInfo":{"status":"ok","timestamp":1647117421552,"user_tz":360,"elapsed":3874,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"8056bb87-0d34-46ec-f97a-8f6efbb912c6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n"]}]},{"cell_type":"code","source":["!cd drive/MyDrive/Repos/gong-metaphor-detection/; bash shell-scripts/run_vua_seq_model.sh\n"],"metadata":{"id":"Gy6a7oIVH0UM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647117543945,"user_tz":360,"elapsed":68701,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"b399e4a6-0841-42cd-8092-6bd6bedd188b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["03/12/2022 20:38:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","Downloading: 100% 482/482 [00:00<00:00, 840kB/s]\n","Downloading: 100% 899k/899k [00:00<00:00, 5.23MB/s]\n","Downloading: 100% 456k/456k [00:00<00:00, 3.13MB/s]\n","Downloading: 100% 1.43G/1.43G [00:32<00:00, 44.1MB/s]\n","03/12/2022 20:38:47 - INFO - modeling_roberta_metaphor -   hidden_size: 1024, pos_dim: 8, feature_dim: 128\n","03/12/2022 20:38:47 - INFO - modeling_roberta_metaphor -   classifier dim: 1032\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMetaphorDetection: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaForMetaphorDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForMetaphorDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForMetaphorDetection were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pos_emb.weight', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/12/2022 20:38:54 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='data/VUA', dataset='VUA', device=device(type='cuda'), do_lower_case=True, do_predict=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, feature_dim=128, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, labels='', learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=256, max_steps=-1, model_name_or_path='roberta-large', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=5.0, output_dir='output/VUA/modeltest/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=18, per_gpu_train_batch_size=6, pos_dim=8, pos_vocab_size=43, save_steps=500, seed=42, server_ip='', server_port='', tokenizer_name='', use_features=False, use_pos=True, warmup_steps=500, weight_decay=0.0)\n","03/12/2022 20:38:54 - INFO - __main__ -   Loading features from cached file data/VUA/cached_train_roberta-large_256\n","class 0: 103571.0, class 1: 13051.0\n","weights: [0.22520203531876684, 1.7871733966745844]\n","train size: 5058, dev_size: 1265\n","03/12/2022 20:39:00 - INFO - __main__ -   ***** Running training *****\n","03/12/2022 20:39:00 - INFO - __main__ -     Num examples = 5058\n","03/12/2022 20:39:00 - INFO - __main__ -     Num Epochs = 5\n","03/12/2022 20:39:00 - INFO - __main__ -     Instantaneous batch size per GPU = 6\n","03/12/2022 20:39:00 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 6\n","03/12/2022 20:39:00 - INFO - __main__ -     Gradient Accumulation steps = 1\n","03/12/2022 20:39:00 - INFO - __main__ -     Total optimization steps = 4215\n","03/12/2022 20:39:00 - INFO - __main__ -    Weighted cross-entropy class weights: [0.22520203531876684, 1.7871733966745844]\n","Epoch:   0% 0/5 [00:00<?, ?it/s]\n","Iteration:   0% 0/843 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","\n","Iteration:   0% 1/843 [00:00<09:01,  1.56it/s]\u001b[A\n","Iteration:   0% 2/843 [00:01<08:39,  1.62it/s]\u001b[A\n","Iteration:   0% 3/843 [00:01<08:28,  1.65it/s]\u001b[A\n","Iteration:   0% 4/843 [00:02<08:22,  1.67it/s]\u001b[A\n","Iteration:   1% 5/843 [00:03<08:18,  1.68it/s]\u001b[A\n","Iteration:   1% 6/843 [00:04<09:29,  1.47it/s]\n","Epoch:   0% 0/5 [00:04<?, ?it/s]\n","Traceback (most recent call last):\n","  File \"run_metaphor_detection.py\", line 757, in <module>\n","    main()\n","  File \"run_metaphor_detection.py\", line 729, in main\n","    class_weights, tokenizer, pad_token_label_id)\n","  File \"run_metaphor_detection.py\", line 193, in train\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 195, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 99, in backward\n","    allow_unreachable=True)  # allow_unreachable flag\n","KeyboardInterrupt\n"]}]}]}